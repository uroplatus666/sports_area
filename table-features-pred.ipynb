{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3c76f4-4086-4548-bbc5-3cba90c251ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.metrics import classification_report, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "import json\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel, pipeline\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abff6284-5933-446f-951c-ebb2300ebb92",
   "metadata": {},
   "source": [
    "### Загружаем данные из гугл карт по спортивным центрам города"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cca5ee-411b-4011-a6d2-1b6014b16808",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = pd.read_excel('-.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5bf38-bc70-4c67-b9f5-a1cd64ae4aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['input_id', 'group',\n",
    "               'build', 'category', 'review_count', 'images']\n",
    "city = city[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9146c26-1ea9-4fde-83f2-8d4b86d633bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in city.columns[1:-2]:\n",
    "    print(col)\n",
    "    print()\n",
    "    print(city[col].value_counts())\n",
    "    print('____________')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ed3ca-f24a-4125-b523-a6fdf1e1c298",
   "metadata": {},
   "source": [
    "### Приведем данные к нужному виду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab45fc-92d9-4699-83fe-1ba1b8181231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_remake_category(value):\n",
    "    value = value.lower()\n",
    "    if value == 'gym' or value == 'fitness center':\n",
    "        return 'gym'\n",
    "    if re.search(r'\\b(fitness)\\b', value):\n",
    "        return 'fitness'\n",
    "    elif re.search(r'\\b(yoga|gymnastic|pilates|health|dance|shaping|dietitian|spa|diabet|sauna|weight|nutritionist)\\b', value):\n",
    "        return 'yoga'\n",
    "    elif re.search(r'\\b(complex|athletic|stadium)\\b', value):\n",
    "        return 'complex'\n",
    "    elif re.search(r'\\b(court)\\b', value):\n",
    "        return 'court'\n",
    "    elif 'school' in value:\n",
    "        return 'school'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def to_remake_group(value):\n",
    "    if 'без' in value:\n",
    "        return 'no_pool'\n",
    "    elif 'бассейн' in value:\n",
    "        return 'pool'\n",
    "    elif 'студия' in value.lower():\n",
    "        return 'studio'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def to_remake_build(value):\n",
    "    if value.lower() == 'ж':\n",
    "        return 'res'\n",
    "    elif value.lower() == 'адм':\n",
    "        return 'adm'\n",
    "    else:\n",
    "        return 'com'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e63b3-8ec0-4c5a-a828-c62f771c50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "city['category'] = city['category'].apply(to_remake_category)\n",
    "city['group'] = city['group'].apply(to_remake_group)\n",
    "city['build'] = city['build'].apply(to_remake_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f26d21-3870-4d5c-8ab3-2aa911d5b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in city.columns[1:-3]:\n",
    "    print(col)\n",
    "    print()\n",
    "    print(city[col].value_counts())\n",
    "    print('____________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a3287-dfb6-4cff-a44a-2481bafc0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_encoded = pd.get_dummies(city, columns=['group', 'build', 'category'])\n",
    "city_encoded = city_encoded[['input_id', 'images',\n",
    "       'group_no_pool', 'group_pool', 'group_studio', 'build_adm', 'build_com', 'build_res', 'category_gym',\n",
    "       'category_school', 'category_yoga', 'category_complex', 'category_fitness',\n",
    "                                 'review_count']]\n",
    "\n",
    "city_encoded[['group_no_pool', 'group_pool', 'group_studio',\n",
    "               'build_adm', 'build_com', 'build_res',\n",
    "               'category_gym','category_school',\n",
    "                'category_yoga', 'category_complex',\n",
    "                'category_fitness',]] = city_encoded.iloc[:,2:-1].astype('int')\n",
    "correlation_matrix = round(city_encoded[['group_no_pool','group_pool',\n",
    "                                           'group_studio', 'build_adm', 'build_com',\n",
    "                                          'build_res', 'category_gym',\n",
    "                                           'category_school', 'category_yoga',\n",
    "                                           'category_complex', 'category_fitness',\n",
    "                                          'review_count']].corr(), 2)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8}, fmt='.2f', linewidths=.5)\n",
    "plt.title('Корреляционная матрица')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fdd480-52ec-4c77-b9cb-b16e96210bec",
   "metadata": {},
   "source": [
    "## Перейдем к классификации спортивных центров на те, что больше 1000 м2 и меньше 1000 м2\n",
    "- `1`: > 1000 м2\n",
    "- `0`: < 1000 м2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680bb34c-1ada-4741-93f7-1befbc1f807b",
   "metadata": {},
   "source": [
    "**Если есть следующие признаки:**\n",
    "\n",
    "- `тип здания`\n",
    "- `количетсво отзывов`\n",
    "- `наличие бассейна`\n",
    "- `студия?`\n",
    "- `категория \"комплекс\"`\n",
    "\n",
    "Воспользуемся `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d398fa-e0ea-4401-9ded-99259cf7ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = load(\"модель//rf_model_no_photo.pkl\")\n",
    "    \n",
    "features = [\n",
    "    'group_pool',\n",
    "    'group_studio',\n",
    "    'build_com',\n",
    "    'build_res',\n",
    "    'category_complex',\n",
    "    'review_count'\n",
    "]\n",
    "\n",
    "y_pred = rf_model.predict(city[features])\n",
    "city['target_class'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538650c8-a96a-4a59-ad5d-3263b4d0c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Обучение\n",
    "\n",
    "# city_encoded = pd.read_excel('data/city_lines_depth.xlsx')\n",
    "# start = city_encoded[city_encoded['target_class']==0].sample(499, random_state = 42)\n",
    "# end = city_encoded[city_encoded['target_class']==1]\n",
    "\n",
    "# df = pd.concat([start, end], axis = 0).reset_index(drop = True)\n",
    "\n",
    "# features = [\n",
    "#     # 'chain',\n",
    "#     # 'group_no_pool',\n",
    "#     'group_pool',\n",
    "#     'group_studio',\n",
    "#     # 'build_adm',\n",
    "#     'build_com',\n",
    "#     'build_res',\n",
    "#     # 'category_gym',\n",
    "#     'category_complex',\n",
    "#     # 'category_yoga',\n",
    "#     'review_count'\n",
    "# ]\n",
    "\n",
    "\n",
    "# X = df[features]\n",
    "# y = df['target_class']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=500, random_state=42, min_samples_leaf=4,\n",
    "#                                warm_start=True, max_features = \"log2\")\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_proba = model.predict_proba(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# roc_auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "# print(f'Recall: {recall:.2f}')\n",
    "# print(f'F1 Score: {f1:.2f}')\n",
    "# print(f'ROC AUC: {roc_auc:.2f}')\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Метрики\n",
    "\n",
    "# # Accuracy: 0.89\n",
    "# # Precision: 0.87\n",
    "# # Recall: 0.91\n",
    "# # F1 Score: 0.89\n",
    "# # ROC AUC: 0.95\n",
    "# # Classification Report:\n",
    "# #               precision    recall  f1-score   support\n",
    "\n",
    "# #            0       0.91      0.87      0.89       155\n",
    "# #            1       0.87      0.91      0.89       145\n",
    "\n",
    "# #     accuracy                           0.89       300\n",
    "# #    macro avg       0.89      0.89      0.89       300\n",
    "# # weighted avg       0.89      0.89      0.89       300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d1c0e-2168-4a40-81ad-d9f20b866b11",
   "metadata": {},
   "source": [
    "### Отфильтруем изображения, указанные в табличке напротив спорт. центров, используя предобученную модель [openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f9a1e-4018-4a8c-aef2-84bb005930ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2249d-3ab1-4ee5-ad79-5a010f89883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# city = pd.read_excel('train//city_encoded.xlsx')\n",
    "\n",
    "def extract_images_and_titles(image_link_str):\n",
    "    try:\n",
    "        image_data = json.loads(image_link_str)\n",
    "        return [item['image'] for item in image_data]\n",
    "    except json.JSONDecodeError:\n",
    "        return []\n",
    "\n",
    "city['images_links'] = city['images'].apply(extract_images_and_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf5ec4-cc71-4848-967b-044df61bb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_correct_links(links):\n",
    "    global links_lst\n",
    "    lst = []\n",
    "    for link in links:\n",
    "        if link.startswith('https:'):\n",
    "            link = link\n",
    "        else:\n",
    "            link = 'https:'+link\n",
    "        links_lst.append(link)\n",
    "        lst.append(link)\n",
    "    return lst\n",
    "    \n",
    "links_lst = []\n",
    "\n",
    "city['images_links'] = city['images_links'].apply(to_correct_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6e14a-42ad-4665-99fd-ce4838ec8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bd3ff-042f-4615-9538-d8126ee53dca",
   "metadata": {},
   "source": [
    "#### Теперь отфильтруем только те, где уверенность в информативном классе более или равна 90 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d342e2-aeef-4abb-ba5a-9759a3d8ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_probs = []\n",
    "lst_useful_links = []\n",
    "for url in links_lst:\n",
    "    image = Image.open(requests.get(url, stream=True).raw)    \n",
    "    inputs = processor(text=[\"indoor gym fitness center yoga studio pilates pool exercise room sports club with exercise equipment yoga mats mirror\",\n",
    "                            \"building outdoors or branding sign or trees or other\"],\n",
    "                       images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    prob = round(float(logits_per_image.softmax(dim=1)[0][0]), 2)\n",
    "    lst_probs.append(prob)\n",
    "    if prob >= 0.9:\n",
    "        lst_useful_links.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9e095-0397-464f-8c73-ba9164aec7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_filter_links(links):\n",
    "    lst = []\n",
    "    for link in links:\n",
    "        if link in lst_useful_links:\n",
    "            lst.append(link)\n",
    "    return lst\n",
    "    \n",
    "city['useful_images'] = city['images_links'].apply(to_filter_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb3158-4769-461a-8232-269571df97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "city.to_excel('train//city_filtered_img.xlsx', index = False)\n",
    "\n",
    "city_useful_img = lst_useful_links.copy()\n",
    "with open('data//city_useful_img.txt', 'w') as file:\n",
    "    for item in city_useful_img:\n",
    "        file.write(f\"{item}\\n\")\n",
    "\n",
    "print(\"Список сохранён в файл city_useful_img.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a894f73-1a95-44b1-91ff-eff029c21d13",
   "metadata": {},
   "source": [
    "### Применим модель, которая описывает картинки [nlpconnect/vit-gpt2-image-captioning](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be206606-c5cc-4b2a-984d-c892c3cd17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(image_str):\n",
    "    if image_str == '[]':\n",
    "        return []\n",
    "    else:\n",
    "        return image_str[2:-2].split(\"', '\")\n",
    "\n",
    "# city_encoded = pd.read_excel('/content/drive/MyDrive/FitnessData/Кейсы/Фитнес_фото/city_smart_depth.xlsx')\n",
    "\n",
    "city_encoded['useful_images'] = city_encoded['useful_images'].apply(convert_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63500cd3-2aad-4593-ac54-f52d1d8a9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658e4eb-defb-4c56-9c49-73f66d9aa0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_text(links):\n",
    "    global total\n",
    "    total += 1\n",
    "    if total % 500 == 0:\n",
    "        print(total)\n",
    "    lst = []\n",
    "    global text_lst\n",
    "    for link in links:\n",
    "        try:\n",
    "            text = pipe(Image.open(requests.get(link, stream=True).raw))[0]['generated_text']\n",
    "            lst.append(text)\n",
    "        except UnidentifiedImageError as e:\n",
    "            print(f\"Could not identify image from {link}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link}: {e}\")\n",
    "            continue\n",
    "    if len(lst) == 0:\n",
    "        text_lst.append([])\n",
    "    else:\n",
    "        text_lst.append(lst)\n",
    "total = 0\n",
    "text_lst = []\n",
    "city_encoded['useful_images'].apply(to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed696950-708f-41f4-bd86-29a4f6158d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_clear(text):\n",
    "    text  =  text.replace('[', ' ')\n",
    "    text  =  text.replace(']', '')\n",
    "    text  =  text.replace(\" '\", \", \")\n",
    "    text  =  text.replace(\" '\", \"\")\n",
    "    text  =  text.replace(\"'\", \"\")\n",
    "    text  =  text.replace(\",, \", \"\")\n",
    "    return text[:-1]\n",
    "    \n",
    "city_encoded['text_descriptions'] = city_encoded['category'] + city_encoded['text_descriptions']\n",
    "city_encoded['text_descriptions'] = city_encoded['text_descriptions'].apply(to_clear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed224d3-e17d-40c5-9f2d-77a9a42061e5",
   "metadata": {},
   "source": [
    "### Применим модель, которая считает глубину [depth-anything/Depth-Anything-V2-Base-hf](https://huggingface.co/depth-anything/Depth-Anything-V2-Base-hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf409471-fc07-4efd-9514-b6607c1bf7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(image_str):\n",
    "    if image_str == '[]':\n",
    "        return []\n",
    "    else:\n",
    "        return image_str[2:-2].split(\"', '\")\n",
    "\n",
    "city_encoded = pd.read_excel('train/city_smart_depth.xlsx')\n",
    "city_encoded['useful_images'] = city_encoded['useful_images'].apply(convert_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b7c54-4792-4f01-839b-3e1ae2c81a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, _, _ = get_backend()\n",
    "checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n",
    "pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199370ad-1414-4b52-9f40-011025e3061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_depth(links):\n",
    "    global total\n",
    "    total += 1\n",
    "    if total % 500 == 0:\n",
    "        print(total)\n",
    "    lst = []\n",
    "    global depth_lst\n",
    "    for link in links:\n",
    "        try:\n",
    "            dictt = pipe(Image.open(requests.get(link, stream=True).raw))\n",
    "            maxx = float(dictt['predicted_depth'].max())\n",
    "            ten = maxx - dictt['predicted_depth']\n",
    "            lst.append(ten)\n",
    "        except UnidentifiedImageError as e:\n",
    "            print(f\"Could not identify image from {link}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link}: {e}\")\n",
    "            continue\n",
    "    depth_lst[total] = lst\n",
    "\n",
    "\n",
    "total = 0\n",
    "depth_lst = {}\n",
    "city_encoded['useful_images'].apply(to_depth)\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bbfd9-71d7-4097-b299-8a3a9970cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/content/drive/MyDrive/FitnessData/Кейсы/Фитнес_фото/depth_tensors_lst.txt', 'w') as file:\n",
    "#     for item in depth_lst:\n",
    "#         file.write(f\"{item}\\n\")\n",
    "\n",
    "# print(\"Список сохранён в файл depth_tensors_lst.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ad418-fef4-4693-a376-25c0339d47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_stats = []\n",
    "for key, values in depth_lst.items():\n",
    "  lst = []\n",
    "  if len(values)>0:\n",
    "    for ten in values:\n",
    "      height = ten.shape[0]\n",
    "      width = ten.shape[1]\n",
    "      maxx = round(float(ten.max()), 4)\n",
    "      floor_14_max = round(float(ten[height//4].max()), 4)\n",
    "      floor_12_max = round(float(ten[height//2].max()), 4)\n",
    "      floor_34_max = round(float(ten[height//4*3].max()), 4)\n",
    "      wall_14_max = round(float(ten[:, width//4].max()), 4)\n",
    "      wall_12_max = round(float(ten[:, width//2].max()), 4)\n",
    "      wall_34_max = round(float(ten[:, width//4*3].max()), 4)\n",
    "\n",
    "      floor_14_p = round(float(len(ten[height//4][ten[height//4]>floor_14_max-1.5])/width), 4)\n",
    "      floor_12_p = round(float(len(ten[height//2][ten[height//2]>floor_12_max-1.5])/width), 4)\n",
    "      floor_34_p = round(float(len(ten[height//4*3][ten[height//4*3]>floor_34_max-1.5])/width), 4)\n",
    "      wall_14_p = round(float(len(ten[:, width//4][ten[:, width//4]>floor_14_max-1.5])/height), 4)\n",
    "      wall_12_p = round(float(len(ten[:, width//2][ten[:, width//2]>floor_12_max-1.5])/height), 4)\n",
    "      wall_34_p = round(float(len(ten[:, width//4*3][ten[:, width//4*3]>floor_34_max-1.5])/height), 4)\n",
    "\n",
    "      floor_14_std = round(float(ten[height//4].std()), 4)\n",
    "      floor_12_std = round(float(ten[height//2].std()), 4)\n",
    "      floor_34_std = round(float(ten[height//4*3].std()), 4)\n",
    "      wall_14_std = round(float(ten[:, width//4].std()), 4)\n",
    "      wall_12_std = round(float(ten[:, width//2].std()), 4)\n",
    "      wall_34_std = round(float(ten[:, width//4*3].std()), 4)\n",
    "\n",
    "      floor_14_sum_norm = round(float(ten[height//4].sum()/width), 4)\n",
    "      floor_12_sum_norm = round(float(ten[height//2].sum()/width), 4)\n",
    "      floor_34_sum_norm = round(float(ten[height//4*3].sum()/width), 4)\n",
    "      wall_14_sum_norm = round(float(ten[:, width//4].sum()/height), 4)\n",
    "      wall_12_sum_norm = round(float(ten[:, width//2].sum()/height), 4)\n",
    "      wall_34_sum_norm = round(float(ten[:, width//4*3].sum()/height), 4)\n",
    "\n",
    "\n",
    "      lst.append([maxx, floor_14_max, floor_12_max, floor_34_max,\n",
    "                  wall_14_max, wall_12_max, wall_34_max,\n",
    "                  floor_14_p, floor_12_p, floor_34_p,\n",
    "                  wall_14_p, wall_12_p, wall_34_p,\n",
    "                  floor_14_std, floor_12_std, floor_34_std,\n",
    "                  wall_14_std, wall_12_std, wall_34_std,\n",
    "                  floor_14_sum_norm, floor_12_sum_norm, floor_34_sum_norm,\n",
    "                  wall_14_sum_norm , wall_12_sum_norm , wall_34_sum_norm])\n",
    "  depth_stats.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b007b-bc82-48de-a4ac-599ff911d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/content/drive/MyDrive/FitnessData/Кейсы/Фитнес_фото/depth_stats_lst.txt', 'w') as file:\n",
    "#     for item in depth_stats:\n",
    "#         file.write(f\"{item}\\n\")\n",
    "\n",
    "# print(\"Список сохранён в файл depth_stats_lst.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b063d4b-e52d-428b-96b1-1200f9352bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "itog = []\n",
    "for line in depth_stats:\n",
    "  if len(line)>0:\n",
    "    id = int(np.argmax(np.array(line)[:,0]))\n",
    "    itog.append(line[id])\n",
    "  else:\n",
    "    itog.append([np.nan for i in range(25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f017b-1bf7-43fe-b5f6-ee53e8630892",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_encoded[['maxx', 'floor_14_max', 'floor_12_max', 'floor_34_max',\n",
    "                  'wall_14_max', 'wall_12_max', 'wall_34_max',\n",
    "                  'floor_14_p', 'floor_12_p', 'floor_34_p',\n",
    "                  'wall_14_p', 'wall_12_p', 'wall_34_p',\n",
    "                 'floor_14_std', 'floor_12_std', 'floor_34_std',\n",
    "                  'wall_14_std', 'wall_12_std', 'wall_34_std',\n",
    "                  'floor_14_sum_norm', 'floor_12_sum_norm', 'floor_34_sum_norm',\n",
    "                  'wall_14_sum_norm', 'wall_12_sum_norm', 'wall_34_sum_norm']] = itog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb676f1-f27f-4dff-8ef7-6fe7e62684be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_encoded.to_excel('/content/drive/MyDrive/FitnessData/Кейсы/Фитнес_фото/city_lines_depth.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedd639-a0af-4503-b2f3-42d2d5ba5880",
   "metadata": {},
   "source": [
    "### После того, как мы посчитали все нужные признаки, решим задачу классификации с помощью `HistGradientBoostingClassifier`, используя с google-maps только количество отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c05b2-522f-4762-9e11-604bb30064ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_model = load(\"модель//rf_bosting_model_photo-reviews.pkl\")\n",
    "vectorizer = load('model/vectorizer_photo-reviews.joblib')\n",
    "\n",
    "features = [\n",
    "    'review_count',\n",
    "    'maxx',\n",
    "    'floor_14_max',\n",
    "    'floor_12_max',\n",
    "    'wall_14_max',\n",
    "    'wall_12_max',\n",
    "    'wall_34_max',\n",
    "    'floor_14_p',\n",
    "    'floor_12_p',\n",
    "    'wall_14_p',\n",
    "    'wall_12_p',\n",
    "    'wall_34_p',\n",
    "    'floor_14_std',\n",
    "    'floor_12_std',\n",
    "    'wall_14_std',\n",
    "    'wall_12_std',\n",
    "    'wall_34_std',\n",
    "    'floor_14_sum_norm',\n",
    "    'floor_12_sum_norm',\n",
    "    'wall_14_sum_norm',\n",
    "    'wall_12_sum_norm',\n",
    "           ]\n",
    "\n",
    "city_text = vectorizer.transform(cityl['text_descriptions'])\n",
    "additional_features_city = csr_matrix(city[features].values)\n",
    "combined_features_city = hstack([city_text, additional_features_city]).toarray()\n",
    "\n",
    "y_pred = boost_model.predict(combined_features_city)\n",
    "city['target_class'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c13def-0c0e-4cc5-bd15-fa98b2990f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# start = city_encoded[city_encoded['target_class'] == 0].sample(499, random_state=42)\n",
    "# end = city_encoded[city_encoded['target_class'] == 1]\n",
    "\n",
    "# # Объединение данных\n",
    "# df = pd.concat([start, end], axis=0).reset_index(drop=True)\n",
    "\n",
    "# features = [\n",
    "#         'review_count',\n",
    "#         'maxx', 'floor_14_max',\n",
    "#         'floor_12_max',\n",
    "#             # 'floor_34_max',\n",
    "#         'wall_14_max',\n",
    "#     'wall_12_max',\n",
    "#         'wall_34_max',\n",
    "#             'floor_14_p',\n",
    "#         'floor_12_p',\n",
    "#             # 'floor_34_p',\n",
    "#         'wall_14_p',\n",
    "#     'wall_12_p',\n",
    "#         'wall_34_p',\n",
    "#             'floor_14_std',\n",
    "#         'floor_12_std',\n",
    "#     # 'floor_34_std',\n",
    "#         'wall_14_std',\n",
    "#     'wall_12_std',\n",
    "#         'wall_34_std',\n",
    "#     'floor_14_sum_norm',\n",
    "#         'floor_12_sum_norm',\n",
    "#     # 'floor_34_sum_norm',\n",
    "#         'wall_14_sum_norm',\n",
    "#     'wall_12_sum_norm',\n",
    "#         # 'wall_34_sum_norm'\n",
    "#            ]\n",
    "# # Предобработка текста\n",
    "# X = df[['text_descriptions',\n",
    "#         'review_count', \n",
    "#         'maxx',\n",
    "#         'floor_14_max',\n",
    "#         'floor_12_max',\n",
    "#         # 'floor_34_max',\n",
    "#         'wall_14_max',\n",
    "#         'wall_12_max',\n",
    "#         'wall_34_max',\n",
    "#         'floor_14_p',\n",
    "#         'floor_12_p',\n",
    "#         # 'floor_34_p',\n",
    "#         'wall_14_p',\n",
    "#         'wall_12_p',\n",
    "#         'wall_34_p',\n",
    "#         'floor_14_std',\n",
    "#         'floor_12_std',\n",
    "#         # 'floor_34_std',\n",
    "#         'wall_14_std',\n",
    "#         'wall_12_std',\n",
    "#         'wall_34_std',\n",
    "#         'floor_14_sum_norm',\n",
    "#         'floor_12_sum_norm',\n",
    "#         # 'floor_34_sum_norm',\n",
    "#         'wall_14_sum_norm',\n",
    "#         'wall_12_sum_norm',\n",
    "#         # 'wall_34_sum_norm'\n",
    "#        ]]\n",
    "# y = df['target_class']\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# stops_eng = set(stopwords.words('english'))\n",
    "\n",
    "# vectorizer = TfidfVectorizer(token_pattern=r'\\b[a-zA-Zа-яА-ЯёЁ]+\\b', \n",
    "#                              stop_words=list(stops_eng), \n",
    "#                              ngram_range=(1, 3), max_df=0.6, min_df=10)\n",
    "\n",
    "# X_train_text = vectorizer.fit_transform(X_train['text_descriptions'])\n",
    "# X_val_text = vectorizer.transform(X_val['text_descriptions'])\n",
    "\n",
    "# additional_features_train = csr_matrix(X_train[features].values)\n",
    "# combined_features_train = hstack([X_train_text, additional_features_train]).toarray()\n",
    "\n",
    "# additional_features_val = csr_matrix(X_val[features].values)\n",
    "# combined_features_val = hstack([X_val_text, additional_features_val]).toarray()\n",
    "\n",
    "# model = HistGradientBoostingClassifier(random_state=42, max_iter=1000,  learning_rate=0.08, l2_regularization=1, min_samples_leaf=12, warm_start=True)\n",
    "# model.fit(combined_features_train, y_train)\n",
    "\n",
    "# y_pred = model.predict(combined_features_val)\n",
    "\n",
    "# y_proba = model.predict_proba(combined_features_val)\n",
    "\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# precision = precision_score(y_val, y_pred)\n",
    "# recall = recall_score(y_val, y_pred)\n",
    "# f1 = f1_score(y_val, y_pred)\n",
    "# roc_auc = roc_auc_score(y_val, y_proba[:, 1])\n",
    "\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "# print(f'Recall: {recall:.2f}')\n",
    "# print(f'F1 Score: {f1:.2f}')\n",
    "# print(f'ROC AUC: {roc_auc:.2f}')\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(y_val, y_pred))\n",
    "# print(f\"\\nВремя выполнения: {execution_time // 60:.2f} минут\")\n",
    "\n",
    "# # Метрики\n",
    "\n",
    "# # Accuracy: 0.85\n",
    "# # Precision: 0.88\n",
    "# # Recall: 0.84\n",
    "# # F1 Score: 0.86\n",
    "# # ROC AUC: 0.91\n",
    "# # Classification Report:\n",
    "# #               precision    recall  f1-score   support\n",
    "\n",
    "# #            0       0.83      0.87      0.85        94\n",
    "# #            1       0.88      0.84      0.86       106\n",
    "\n",
    "# #     accuracy                           0.85       200\n",
    "# #    macro avg       0.85      0.86      0.85       200\n",
    "# # weighted avg       0.86      0.85      0.86       200\n",
    "\n",
    "\n",
    "# # Время выполнения: 0.00 минут"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6d5dc-1f6f-47d5-aa67-2ae823339e88",
   "metadata": {},
   "source": [
    "### Решим задачу классификации с помощью `HistGradientBoostingClassifier`, используя с google-maps количество отзывов, информацию о наличии бассейна и студия это или нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e35bb-b2be-4e54-8760-859280ba7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_model = load(\"модель//rf_bosting_model_photo-reviews-ps.pkl\")\n",
    "vectorizer = load('model/vectorizer_photo-reviews-ps.joblib')\n",
    "\n",
    "features = [\n",
    "    'group_pool',\n",
    "    'group_studio',\n",
    "    'review_count',\n",
    "    'maxx',\n",
    "    'floor_14_max',\n",
    "    'floor_12_max',\n",
    "    'wall_14_max',\n",
    "    'wall_12_max',\n",
    "    'wall_34_max',\n",
    "    'floor_14_p',\n",
    "    'floor_12_p',\n",
    "    'wall_14_p',\n",
    "    'wall_12_p',\n",
    "    'wall_34_p',\n",
    "    'floor_14_std',\n",
    "    'floor_12_std',\n",
    "    'wall_14_std',\n",
    "    'wall_12_std',\n",
    "    'wall_34_std',\n",
    "    'floor_14_sum_norm',\n",
    "    'floor_12_sum_norm',\n",
    "    'wall_14_sum_norm',\n",
    "    'wall_12_sum_norm',\n",
    "           ]\n",
    "\n",
    "city_text = vectorizer.transform(cityl['text_descriptions'])\n",
    "additional_features_city = csr_matrix(city[features].values)\n",
    "combined_features_city = hstack([city_text, additional_features_city]).toarray()\n",
    "\n",
    "y_pred = boost_model.predict(combined_features_city)\n",
    "city['target_class'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b535e-5d74-4f73-9fb4-a11095aea3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# start = city_encoded[city_encoded['target_class'] == 0].sample(499, random_state=42)\n",
    "# end = city_encoded[city_encoded['target_class'] == 1]\n",
    "\n",
    "# df = pd.concat([start, end], axis=0).reset_index(drop=True)\n",
    "\n",
    "# features = [\n",
    "#     'group_pool',\n",
    "#     'group_studio',\n",
    "#     'review_count',\n",
    "#     'maxx',\n",
    "#     'floor_14_max',\n",
    "#     'floor_12_max',\n",
    "#     # 'floor_34_max',\n",
    "#     'wall_14_max',\n",
    "#     'wall_12_max',\n",
    "#     'wall_34_max',\n",
    "#     'floor_14_p',\n",
    "#     'floor_12_p',\n",
    "#     # 'floor_34_p',\n",
    "#     'wall_14_p',\n",
    "#     'wall_12_p',\n",
    "#     'wall_34_p',\n",
    "#     'floor_14_std',\n",
    "#     'floor_12_std',\n",
    "#     # 'floor_34_std',\n",
    "#     'wall_14_std',\n",
    "#     'wall_12_std',\n",
    "#     'wall_34_std',\n",
    "#     'floor_14_sum_norm',\n",
    "#     'floor_12_sum_norm',\n",
    "#     # 'floor_34_sum_norm',\n",
    "#     'wall_14_sum_norm',\n",
    "#     'wall_12_sum_norm',\n",
    "#     # 'wall_34_sum_norm'\n",
    "#            ]\n",
    "\n",
    "# X = df[['text_descriptions',\n",
    "#         'group_pool',\n",
    "#         'group_studio',\n",
    "#         'review_count', \n",
    "#         'maxx',\n",
    "#         'floor_14_max',\n",
    "#         'floor_12_max',\n",
    "#         # 'floor_34_max',\n",
    "#         'wall_14_max',\n",
    "#         'wall_12_max',\n",
    "#         'wall_34_max',\n",
    "#         'floor_14_p',\n",
    "#         'floor_12_p',\n",
    "#         # 'floor_34_p',\n",
    "#         'wall_14_p',\n",
    "#         'wall_12_p',\n",
    "#         'wall_34_p',\n",
    "#         'floor_14_std',\n",
    "#         'floor_12_std',\n",
    "#         # 'floor_34_std',\n",
    "#         'wall_14_std',\n",
    "#         'wall_12_std',\n",
    "#         'wall_34_std',\n",
    "#         'floor_14_sum_norm',\n",
    "#         'floor_12_sum_norm',\n",
    "#         # 'floor_34_sum_norm',\n",
    "#         'wall_14_sum_norm',\n",
    "#         'wall_12_sum_norm',\n",
    "#         # 'wall_34_sum_norm'\n",
    "#        ]]\n",
    "# y = df['target_class']\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# stops_eng = set(stopwords.words('english'))\n",
    "\n",
    "# vectorizer = TfidfVectorizer(token_pattern=r'\\b[a-zA-Zа-яА-ЯёЁ]+\\b', \n",
    "#                              stop_words=list(stops_eng), \n",
    "#                              ngram_range=(1, 3), max_df=0.6, min_df=10)\n",
    "\n",
    "# X_train_text = vectorizer.fit_transform(X_train['text_descriptions'])\n",
    "# X_val_text = vectorizer.transform(X_val['text_descriptions'])\n",
    "\n",
    "# additional_features_train = csr_matrix(X_train[features].values)\n",
    "# combined_features_train = hstack([X_train_text, additional_features_train]).toarray()\n",
    "\n",
    "# additional_features_val = csr_matrix(X_val[features].values)\n",
    "# combined_features_val = hstack([X_val_text, additional_features_val]).toarray()\n",
    "\n",
    "# model = HistGradientBoostingClassifier(random_state=42, max_iter=1000,  learning_rate=0.08, l2_regularization=1, min_samples_leaf=12, warm_start=True)\n",
    "# model.fit(combined_features_train, y_train)\n",
    "\n",
    "# y_pred = model.predict(combined_features_val)\n",
    "\n",
    "# y_proba = model.predict_proba(combined_features_val)\n",
    "\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# precision = precision_score(y_val, y_pred)\n",
    "# recall = recall_score(y_val, y_pred)\n",
    "# f1 = f1_score(y_val, y_pred)\n",
    "# roc_auc = roc_auc_score(y_val, y_proba[:, 1])\n",
    "\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "# print(f'Recall: {recall:.2f}')\n",
    "# print(f'F1 Score: {f1:.2f}')\n",
    "# print(f'ROC AUC: {roc_auc:.2f}')\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(y_val, y_pred))\n",
    "# print(f\"\\nВремя выполнения: {execution_time // 60:.2f} минут\")\n",
    "\n",
    "# # Метрики\n",
    "\n",
    "# # Accuracy: 0.85\n",
    "# # Precision: 0.87\n",
    "# # Recall: 0.84\n",
    "# # F1 Score: 0.86\n",
    "# # ROC AUC: 0.93\n",
    "# # Classification Report:\n",
    "# #               precision    recall  f1-score   support\n",
    "\n",
    "# #            0       0.83      0.86      0.84        94\n",
    "# #            1       0.87      0.84      0.86       106\n",
    "\n",
    "# #     accuracy                           0.85       200\n",
    "# #    macro avg       0.85      0.85      0.85       200\n",
    "# # weighted avg       0.85      0.85      0.85       200\n",
    "\n",
    "\n",
    "# # Время выполнения: 1.00 минут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf52f73-13a6-41ac-9450-5c9e1fb3af77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
